{"cells":[{"metadata":{},"cell_type":"markdown","source":"https://medium.com/sicara/keras-tutorial-content-based-image-retrieval-convolutional-denoising-autoencoder-dc91450cc511","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"https://www.pyimagesearch.com/2020/03/30/autoencoders-for-content-based-image-retrieval-with-keras-and-tensorflow/","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"https://blog.keras.io/building-autoencoders-in-keras.html","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\nfrom keras.models import Model\nfrom keras.callbacks import TensorBoard\nfrom keras.datasets import mnist\nimport numpy as np\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\nx_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format\n\nnoise_factor = 0.5\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n\n\ndef train_model():\n    input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n    encoded = MaxPooling2D((2, 2), padding='same', name='encoder')(x)\n\n    # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n\n    x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(16, (3, 3), activation='relu')(x)\n    x = UpSampling2D((2, 2))(x)\n    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\n    autoencoder = Model(input_img, decoded)\n    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n\n    autoencoder.fit(x_train_noisy, x_train,\n                    epochs=20,\n                    batch_size=128,\n                    shuffle=True,\n                    validation_data=(x_test_noisy, x_test),\n                    callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False)])\n\n    autoencoder.save('autoencoder.h5')\n\ntrain_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom keras.models import Model\nfrom keras.datasets import mnist\nimport cv2\nfrom keras.models import load_model\nfrom sklearn.metrics import label_ranking_average_precision_score\nimport time\n\nprint('Loading mnist dataset')\nt0 = time.time()\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\nx_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format\n\nnoise_factor = 0.5\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\nt1 = time.time()\nprint('mnist dataset loaded in: ', t1-t0)\n\nprint('Loading model :')\nt0 = time.time()\n# Load previously trained autoencoder\nautoencoder = load_model('autoencoder.h5')\nt1 = time.time()\nprint('Model loaded in: ', t1-t0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_denoised_images():\n    denoised_images = autoencoder.predict(x_test_noisy.reshape(x_test_noisy.shape[0], x_test_noisy.shape[1], x_test_noisy.shape[2], 1))\n    test_img = x_test_noisy[0]\n    resized_test_img = cv2.resize(test_img, (280, 280))\n    cv2.imshow('input', resized_test_img)\n    cv2.waitKey(0)\n    output = denoised_images[0]\n    resized_output = cv2.resize(output, (280, 280))\n    cv2.imshow('output', resized_output)\n    cv2.waitKey(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_denoised_images()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef compute_average_precision_score(test_codes, test_labels, learned_codes, y_train, n_samples):\n    # For each n_samples (=number of retrieved images to assess) we store the corresponding labels and distances\n    out_labels = []\n    out_distances = []\n    \n    #For each query image feature we compute the closest images from training dataset\n    for i in range(len(test_codes)):\n        distances = []\n        # Compute the euclidian distance for each feature from training dataset\n        for code in learned_codes:\n            distance = np.linalg.norm(code - test_codes[i])\n            distances.append(distance)\n        \n        # Store the computed distances and corresponding labels from training dataset\n        distances = np.array(distances)\n        \n        # Scoring function needs to replace similar labels by 1 and different ones by 0\n        labels = np.copy(y_train).astype('float32')\n        labels[labels != test_labels[i]] = -1\n        labels[labels == test_labels[i]] = 1\n        labels[labels == -1] = 0\n        distance_with_labels = np.stack((distances, labels), axis=-1)\n        sorted_distance_with_labels = distance_with_labels[distance_with_labels[:, 0].argsort()]\n        \n        # The distances are between 0 and 28. The lesser the distance the bigger the relevance score should be\n        sorted_distances = 28 - sorted_distance_with_labels[:, 0]\n        sorted_labels = sorted_distance_with_labels[:, 1]\n        \n        # We keep only n_samples closest elements from the images retrieved\n        out_distances.append(sorted_distances[:n_samples])\n        out_labels.append(sorted_labels[:n_samples])\n\n    out_labels = np.array(out_labels)\n    out_labels_file_name = 'computed_data/out_labels_{}'.format(n_samples)\n    np.save(out_labels_file_name, out_labels)\n\n    out_distances_file_name = 'computed_data/out_distances_{}'.format(n_samples)\n    out_distances = np.array(out_distances)\n    np.save(out_distances_file_name, out_distances)\n    \n    # Score the model based on n_samples first images retrieved\n    score = label_ranking_average_precision_score(out_labels, out_distances)\n    scores.append(score)\n    return score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import label_ranking_average_precision_score\n\n# In our use case 1 means that the image is relevant (same label as the query image) \n# And 0 means that the image is irrelevant\ny_true = np.array([[1, 1, 0, 0]])\n\n# For each train image we compute the relevance score\n\n''' Example 1 '''\ny_score = np.array([[28, 10, 1, 0.5]])\nlabel_ranking_average_precision_score(y_true, y_score)\n# In this first example, the two relevant items have the highest score,\n# the scoring function returns 1.0\n\n''' Example 2'''\ny_score = np.array([[28, 10, 10, 0.5]])\nlabel_ranking_average_precision_score(y_true, y_score)\n# returns 0.83333333333333326\n\n''' Example 3'''\ny_score = np.array([[28, 10, 28, 0.5]])\nlabel_ranking_average_precision_score(y_true, y_score)\n# returns 0.58333333333333326\n\n''' Example 4'''\ny_score = np.array([[10, 10, 28, 28]])\nlabel_ranking_average_precision_score(y_true, y_score)\n# returns 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport numpy as np\nfrom keras.models import Model\nfrom keras.datasets import mnist\nfrom keras.models import load_model\nfrom sklearn.metrics import label_ranking_average_precision_score\n\n# Load mnist dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Load previsouly trained model\nautoencoder = load_model('autoencoder.h5')\n\n# Get encoder layer from trained model\nencoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encoder').output)\n\n# Array in which we will store computed scores\nscores = []\n\n# In order to save time on computations we keep only 1000 query images from test dataset \nn_test_samples = 1000\n\n# Each time we will score the first 10 retrieved images, then the first 50 etc...\nn_train_samples = [10, 50, 100, 200, 300, 400, 500, 750, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000,\n                   20000, 30000, 40000, 50000, 60000]\n\n\ndef test_model(n_test_samples, n_train_samples):\n    # Compute features for training dataset\n    learned_codes = encoder.predict(x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))\n    learned_codes = learned_codes.reshape(learned_codes.shape[0], learned_codes.shape[1] * learned_codes.shape[2] * learned_codes.shape[3])\n    \n    # Compute features for query images\n    test_codes = encoder.predict(x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1))\n    test_codes = test_codes.reshape(test_codes.shape[0], test_codes.shape[1] * test_codes.shape[2] * test_codes.shape[3])\n    \n    # We keep only n_test_samples query images from test dataset\n    indexes = np.arange(len(y_test))\n    np.random.shuffle(indexes)\n    indexes = indexes[:n_test_samples]\n    \n    # Compute score\n    score = compute_average_precision_score(test_codes[indexes], y_test[indexes], learned_codes, y_train, n_train_samples)\n\nfor n_train_sample in n_train_samples:\n    test_model(n_test_samples, n_train_sample)\n \n# Save the computed scores into a file\nnp.save('computed_data/scores', np.array(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}