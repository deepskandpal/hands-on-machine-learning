# -*- coding: utf-8 -*-
"""test colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/deepskandpal/hands-on-machine-learning/blob/POC/Newyork_taxi_ETA.ipynb
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

from google.colab import files
files.upload()

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle
!chmod 600 /root/.kaggle/kaggle.json

!kaggle competitions download -c nyc-taxi-trip-duration

!ls

train_data = pd.read_csv("train.zip")
train_data.head()

train_data.info()

train_data.describe()

from geopy.geocoders import Nominatim
from geopy.distance import geodesic
geolocator = Nominatim(user_agent="my_app")
location = geolocator.reverse("40.767937, -73.982155")
location1 = (40.767937, -73.982155)
location2 = (40.765602, -73.964630)
addr1 = geolocator.reverse("40.767937, -73.982155")
addr2 = geolocator.reverse("40.765602, -73.964630")
print("addr1 =", addr1.address)
print("addr2 = ", addr2.address)
print(geodesic(location1, location2).m)

def get_distance_in_metre(df):
  start_point = (df["pickup_latitude"], df["pickup_longitude"])
  drop_point = (df["dropoff_latitude"], df["dropoff_longitude"])
  total_distance = geodesic(start_point, drop_point).m
  return total_distance

train_data['distance'] = train_data.apply(get_distance_in_metre, axis = 1)

train_data_cpy = train_data.drop(["dropoff_longitude", "dropoff_latitude", "pickup_latitude", "pickup_longitude"], axis=1)

train_data_cpy

train_data_cpy['pickup_datetime'] = pd.to_datetime(train_data_cpy['pickup_datetime'])
train_data_cpy['dropoff_datetime'] = pd.to_datetime(train_data_cpy['dropoff_datetime'])

def datetime_to_float(d):
    return d.timestamp()
temp = train_data_cpy['pickup_datetime'].apply(datetime_to_float)

train_data_cpy = train_data_cpy.drop(['id'], axis=1)

def datetime_to_float(d):
    return d.timestamp()

def normalize_numeric_data(data):
    y = data['trip_duration']
    y_train = np.asarray(y).astype('float32')
    data = data.drop(['trip_duration'], axis=1)
    date_time = data.select_dtypes(include=['datetime64'])
    for column in date_time:
        date_time[column] = date_time[column].apply(datetime_to_float)
    numeric_data = data.select_dtypes(include=[np.number])
    from sklearn import preprocessing
    x = numeric_data.values #returns a numpy array
    min_max_scaler = preprocessing.MinMaxScaler()
    x_scaled = min_max_scaler.fit_transform(x)
    df = pd.DataFrame(x_scaled)
    df_col = pd.concat([date_time,df], axis=1)
    #np_reader = tf.contrib.timeseries.NumpyReader(data={tf.contrib.timeseries.TrainEvalFeatures.TIMES: data['timestamp'].values, tf.contrib.timeseries.TrainEvalFeatures.VALUES : data['value'].values})
    X = df_col.to_numpy()
    #X = np.asarray(X).astype(np.float32)
    return X, y_train

X, y = normalize_numeric_data(train_data)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)

from keras import models
from keras import layers

def build_model():
    model = models.Sequential() 
    model.add(layers.Dense(512, activation='selu', input_shape=(X_train.shape[1],))) 
    model.add(layers.Dropout(rate=0.5))
    model.add(layers.Dense(512, activation='selu'))
    model.add(layers.Dropout(rate=0.5))
    model.add(layers.Dense(512, activation='selu'))
    model.add(layers.Dropout(rate=0.5))
    model.add(layers.Dense(512, activation='selu'))
    model.add(layers.Dropout(rate=0.5))
    model.add(layers.Dense(512, activation='selu'))
    model.add(layers.Dropout(rate=0.5))
    model.add(layers.Dense(512, activation='selu'))
    model.add(layers.Dropout(rate=0.5))
    model.add(layers.Dense(512, activation='selu'))
    model.add(layers.Dropout(rate=0.5))
    model.add(layers.Dense(256, activation='selu'))
    model.add(layers.Dropout(rate=0.5))
    model.add(layers.Dense(256, activation='selu'))
    model.add(layers.Dropout(rate=0.5))
    model.add(layers.Dense(256, activation='selu'))
    model.add(layers.Dropout(rate=0.5))
    model.add(layers.Dense(128, activation='selu'))
    model.add(layers.Dropout(rate=0.5))
    model.add(layers.Dense(1))
    model.compile(optimizer='rmsprop', loss='mean_squared_logarithmic_error', metrics=['acc']) 
    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=20, batch_size=12, verbose=1)
    return history

history = build_model()