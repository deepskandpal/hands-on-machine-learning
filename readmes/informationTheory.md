# Information Theory

### Signals and Noise

1. Entropy 
2. Cross Entropy
3. Signal to noise ratio


### Where it is used

pointers

information theory
transmission of information
likely event less information
unlikely event more information
expectation of an event to transmit the average amount of information
non uniform distribution has smaller length code
entropy 
entropy from physics to statsitical mechanics
entropy of discrete random variable
entropy of continous random variable
relative entropy, mutual information and KL divergence 
convex and concave functions

Kl divergence entropy and cross entropy for machine learning link between Kl divergence and maximum likelihood


### More links to read up